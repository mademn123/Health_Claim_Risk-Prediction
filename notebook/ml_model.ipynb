{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58a5a886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Stats\n",
      "Class balance (positive rate): 0.860\n",
      "Accuracy    : 0.848\n",
      "ROC-AUC     : 0.828\n",
      "Brier score : 0.177\n",
      "Precision   : 0.912  Recall: 0.910  F1: 0.911\n",
      "Confusion matrix:\n",
      " [[ 255  285]\n",
      " [ 291 2953]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, brier_score_loss,\n",
    "    precision_recall_fscore_support, confusion_matrix\n",
    ")\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import GammaRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "\n",
    "# 1st Model: Frequency Model (probability of claim vs no claim)\n",
    "\n",
    "# load the dataset from the csv file using pandas to read it\n",
    "df = pd.read_csv(\"../data/h251.csv\")\n",
    "\n",
    "# chose target variable to be total expenditures (total amount of money a person spent on healthcare in 2023)\n",
    "target = \"TOTEXP23\"\n",
    "\n",
    "# chose predictors to train the model on\n",
    "# predictors are age, sex, race, region, cancer, high blood pressure, asthma, coronary heart disease, insurance coverage\n",
    "predictors = [\n",
    "    # demographics\n",
    "    \"AGE23X\", \"SEX\", \"RACETHX\", \"REGION23\", \"MARRY23X\", \n",
    "    # socioeconomic\n",
    "    \"POVCAT23\", \"INSCOV23\", \n",
    "    # illnesses / chronic conditions\n",
    "    \"CANCERDX\", \"HIBPDX\", \"ASTHDX\", \"CHDDX\", \"DIABDX_M18\", \n",
    "    # lifestyle\n",
    "    \"ADSMOK42\"\n",
    "]\n",
    "\n",
    "# concatenate each line of predictors with the target value and drop any missing values\n",
    "df = df[predictors + [target]].dropna()\n",
    "\n",
    "# changes the 2 to be 0 so that it works as binary values for the model\n",
    "for c in [\"CANCERDX\",\"HIBPDX\",\"ASTHDX\",\"CHDDX\", \"DIABDX_M18\"]:\n",
    "    if df[c].dtype != 'float' and df[c].dtype != 'int':\n",
    "        pass\n",
    "    df[c] = df[c].map({1:1, 2:0}).fillna(df[c])\n",
    "\n",
    "# assign X to be the set of predictors\n",
    "X = df[predictors]\n",
    "# assign y to be the target; converts the value into a binary so that if the total expenditures is >= 1, \n",
    "# it's 1, otherwise it's 0\n",
    "y = (df[\"TOTEXP23\"] > 0).astype(int)\n",
    "\n",
    "# splits dataset to train 80% and test 20%, set random_state to an integer so that it controls the randomness of how it splits the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
    "\n",
    "categorical_cols = [\"SEX\", \"RACETHX\", \"REGION23\", \"MARRY23X\", \"POVCAT23\", \"INSCOV23\", \"ADSMOK42\"]\n",
    "numerical_cols = [\"AGE23X\", \"CANCERDX\", \"HIBPDX\", \"ASTHDX\", \"CHDDX\", \"DIABDX_M18\"]\n",
    "\n",
    "# data preprocessing since LogisticRegression will mistake the categorical values for numerical values\n",
    "# use OneHotEncoder to transform each category into a binary column \n",
    "# use StandardScaler scale the numerical values\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, drop=None), categorical_cols),\n",
    "        (\"num\", StandardScaler(), numerical_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocess.fit(X_train)\n",
    "\n",
    "# using a pipeline so that preprocessing happens before fitting the data to the LogisticRegression model\n",
    "base_clf = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", LogisticRegression(max_iter=5000, class_weight=\"balanced\", solver=\"liblinear\"))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"model__C\": [0.1, 0.5, 1.0, 2.0, 5.0],\n",
    "    \"model__penalty\": [\"l1\", \"l2\"]\n",
    "}\n",
    "clf = GridSearchCV(base_clf, param_grid, cv=5, scoring=\"roc_auc\", n_jobs=-1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# predict probabilities and classify with threshold 0.3\n",
    "proba = clf.predict_proba(X_test)[:,1]\n",
    "y_hat = (proba >= 0.3).astype(int)\n",
    "\n",
    "# calculates metrics for logistic regression\n",
    "acc  = accuracy_score(y_test, y_hat)\n",
    "auc  = roc_auc_score(y_test, proba)\n",
    "brier = brier_score_loss(y_test, proba)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_hat, average=\"binary\")\n",
    "cm = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print(f\"Logistic Regression Model Stats\")\n",
    "print(f\"Class balance (positive rate): {y.mean():.3f}\")\n",
    "print(f\"Accuracy    : {acc:.3f}\")\n",
    "print(f\"ROC-AUC     : {auc:.3f}\")\n",
    "print(f\"Brier score : {brier:.3f}\")\n",
    "print(f\"Precision   : {prec:.3f}  Recall: {rec:.3f}  F1: {f1:.3f}\")\n",
    "print(\"Confusion matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29eaa001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HistGradientBoostingClassifier Model Stats\n",
      "Accuracy    : 0.872\n",
      "ROC-AUC     : 0.834\n",
      "Brier score : 0.096\n",
      "Precision   : 0.888  Recall: 0.974  F1: 0.929\n",
      "Confusion matrix:\n",
      " [[ 142  398]\n",
      " [  85 3159]]\n"
     ]
    }
   ],
   "source": [
    "# HistGradientBoostingClassifier Model\n",
    "improved_clf = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", HistGradientBoostingClassifier(\n",
    "        learning_rate=0.1,\n",
    "        max_depth=None,\n",
    "        max_iter=300,\n",
    "        l2_regularization=0.0,\n",
    "        early_stopping=True,\n",
    "        random_state=23\n",
    "    ))\n",
    "])\n",
    "\n",
    "improved_clf.fit(X_train, y_train)\n",
    "\n",
    "proba_improved = improved_clf.predict_proba(X_test)[:, 1]\n",
    "yhat_improved = (proba_improved >= 0.5).astype(int)\n",
    "\n",
    "# metrics for HistGradientBoostingClassifier\n",
    "acc   = accuracy_score(y_test, yhat_improved)\n",
    "auc   = roc_auc_score(y_test, proba_improved)\n",
    "brier = brier_score_loss(y_test, proba_improved)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_test, yhat_improved, average=\"binary\")\n",
    "cm = confusion_matrix(y_test, yhat_improved)\n",
    "\n",
    "print()\n",
    "print(\"HistGradientBoostingClassifier Model Stats\")\n",
    "print(f\"Accuracy    : {acc:.3f}\")\n",
    "print(f\"ROC-AUC     : {auc:.3f}\")\n",
    "print(f\"Brier score : {brier:.3f}\")\n",
    "print(f\"Precision   : {prec:.3f}  Recall: {rec:.3f}  F1: {f1:.3f}\")\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "# results show that the HistGradientBoostingClassifier model predicts probabilities more accurately, \n",
    "# identifies almost all spenders, and has a higher accuracy, but it does make more false positives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbcac48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GammaRegressor Model Stats\n",
      "MAE  : 9,752.80\n",
      "RMSE : 20,661.76\n",
      "R^2  : 0.051\n"
     ]
    }
   ],
   "source": [
    "# 2nd model: Severity Model (cost)\n",
    "\n",
    "# only include people who have spent money\n",
    "df_copy = df[df[\"TOTEXP23\"] > 0].copy()\n",
    "X2 = df_copy[predictors]\n",
    "y2 = df_copy[\"TOTEXP23\"]\n",
    "\n",
    "\n",
    "preprocess_reg = ColumnTransformer([\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_cols),\n",
    "    (\"num\", StandardScaler(), numerical_cols),\n",
    "])\n",
    "\n",
    "# using a pipeline so that preprocessing happens before fitting the data to the GammaRegressor\n",
    "base_reg = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess_reg),\n",
    "    (\"model\", GammaRegressor(max_iter=5000))\n",
    "])\n",
    "\n",
    "# splits dataset to train 80% and test 20%, set random_state to an integer so that it controls the randomness of how it splits the dataset\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=23)\n",
    "\n",
    "base_reg.fit(X2_train, y2_train)\n",
    "\n",
    "y2_pred = base_reg.predict(X2_test)\n",
    "\n",
    "# metrics for gamma regressor\n",
    "print(\"GammaRegressor Model Stats\")\n",
    "print(f\"MAE  : {mean_absolute_error(y2_test, y2_pred):,.2f}\")\n",
    "print(f\"RMSE : {root_mean_squared_error(y2_test, y2_pred):,.2f}\")\n",
    "print(f\"R^2  : {r2_score(y2_test, y2_pred):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ce74279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HistGradientBoostingRegressor Model\n",
      "MAE  : 9,787.61\n",
      "RMSE : 20,398.31\n",
      "R^2  : 0.075\n"
     ]
    }
   ],
   "source": [
    "#HistGradientBoostingRegressor\n",
    "improved_reg = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess_reg),\n",
    "    (\"model\", HistGradientBoostingRegressor(\n",
    "        learning_rate=0.05,\n",
    "        max_iter=400,\n",
    "        max_depth=None,\n",
    "        min_samples_leaf=20,\n",
    "        l2_regularization=0.0,\n",
    "        early_stopping=True,\n",
    "        random_state=23\n",
    "    ))\n",
    "])\n",
    "\n",
    "improved_reg.fit(X2_train, y2_train)\n",
    "y2_pred = improved_reg.predict(X2_test)\n",
    "\n",
    "# ensures that costs are nonnegative\n",
    "y2_pred = np.clip(y2_pred, 0, None)\n",
    "\n",
    "# metrics for HistGradientBoosting Regressor\n",
    "mae  = mean_absolute_error(y2_test, y2_pred)\n",
    "rmse = root_mean_squared_error(y2_test, y2_pred)\n",
    "r2   = r2_score(y2_test, y2_pred)\n",
    "\n",
    "print(\"HistGradientBoostingRegressor Model\")\n",
    "print(f\"MAE  : {mae:,.2f}\")\n",
    "print(f\"RMSE : {rmse:,.2f}\")\n",
    "print(f\"R^2  : {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e409be49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk band cutoffs:\n",
      "  Low < $4,784   Medium < $12,505   High ≥ $12,505\n"
     ]
    }
   ],
   "source": [
    "# Estimated Annual Cost Model\n",
    "# created by combining the frequency and severity model (by multiplying them)\n",
    "# EC = p x m where p is the probability of spending and m is the average cost spent\n",
    "\n",
    "# probability of any spending\n",
    "p_all = improved_clf.predict_proba(df[predictors])[:, 1]\n",
    "\n",
    "# conditional expected cost given that spending is greater than 0\n",
    "m_all = improved_reg.predict(df[predictors])\n",
    "m_all = np.clip(m_all, 0, None)     # to ensure that costs aren't negative\n",
    "\n",
    "# expected annual cost\n",
    "df[\"EST_COST\"] = p_all * m_all\n",
    "\n",
    "# risk bands to divide population into quantiles of expected cost\n",
    "# < 40th percentile: \"Low\", 40th-80th percentile: \"Medium\", > 80th percentile: \"High\"\n",
    "low_cut, med_cut = np.quantile(df[\"EST_COST\"], [0.40, 0.80])\n",
    "\n",
    "def band(v):\n",
    "    if v < low_cut: return \"Low\"\n",
    "    elif v < med_cut: return \"Medium\"\n",
    "    else: return \"High\"\n",
    "\n",
    "df[\"RISK_BAND\"] = df[\"EST_COST\"].apply(band)\n",
    "\n",
    "# prints the cutoff dollar values for each risk band\n",
    "print(\"Risk band cutoffs:\")\n",
    "print(f\"  Low < ${low_cut:,.0f}   Medium < ${med_cut:,.0f}   High ≥ ${med_cut:,.0f}\")\n",
    "\n",
    "# maps the user inputted data to the correct MEPS codes\n",
    "REGION_MAP = {\"northeast\":1, \"midwest\":2, \"south\":3, \"west\":4}\n",
    "SEX_MAP    = {\"male\":1, \"female\":2}\n",
    "INSCOV_MAP = {\"private\":1, \"public\":2, \"uninsured\":3}\n",
    "RACE_MAP = {\"white\": 1, \"black\": 2, \"hispanic\": 3, \"asian\": 4, \"other\": 5}\n",
    "\n",
    "# converts the yes/no into bool values of 0 or 1\n",
    "def to_code_bool(v):\n",
    "    if isinstance(v, str):\n",
    "        v = v.strip().lower()\n",
    "        return 1 if v in {\"1\",\"y\",\"yes\",\"true\",\"t\"} else 0\n",
    "    return 1 if bool(v) else 0\n",
    "\n",
    "def make_row(age, sex, region, inscov, racethx,\n",
    "             marry23x=1, povcat23=1, adsmok42=2, diabdx=False,\n",
    "             cancer=False, hbp=False, asthma=False, chd=False):\n",
    "\n",
    "    row = {\n",
    "        \"AGE23X\": age,\n",
    "        \"SEX\": SEX_MAP.get(str(sex).lower(), sex),\n",
    "        \"RACETHX\": RACE_MAP.get(str(racethx).lower(), racethx),\n",
    "        \"REGION23\": REGION_MAP.get(str(region).lower(), region),\n",
    "        \"MARRY23X\": marry23x,     \n",
    "        \"POVCAT23\": povcat23,       \n",
    "        \"INSCOV23\": INSCOV_MAP.get(str(inscov).lower(), inscov),\n",
    "        \"CANCERDX\": to_code_bool(cancer),\n",
    "        \"HIBPDX\":   to_code_bool(hbp),\n",
    "        \"ASTHDX\":   to_code_bool(asthma),\n",
    "        \"CHDDX\":    to_code_bool(chd),\n",
    "        \"DIABDX_M18\": to_code_bool(diabdx),  \n",
    "        \"ADSMOK42\": adsmok42,     \n",
    "    }\n",
    "\n",
    "    return pd.DataFrame([row], columns=predictors)\n",
    "\n",
    "# takes in the user's info and runs both models with it\n",
    "def estimate_cost(age, sex, region, inscov, cancer=False, hbp=False, asthma=False, chd=False, racethx=None,\n",
    "                  threshold=0.5):\n",
    "    \"\"\"Return p_i, m_i, EC, and risk band for a single user profile.\"\"\"\n",
    "    X_new = make_row(age, sex, region, inscov, cancer, hbp, asthma, chd, racethx)\n",
    "    p = float(improved_clf.predict_proba(X_new)[:, 1])              # probability of any spending\n",
    "    m = float(np.clip(improved_reg.predict(X_new)[0], 0, None))     # expected cost given spending\n",
    "    ec = p * m                                                      # expected annual cost\n",
    "    rb = band(ec)                                                   # risk band\n",
    "    return {\"p_any_spend\": p, \"cond_mean_cost\": m, \"est_cost\": ec, \"risk_band\": rb}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
